## xlaclient-rs

In reinforcement learning (RL), one often designs a *policy network* 
for an *agent*, which outputs a probability distribution over the actions the 
agent can take, given the state of the world, hopefully enabling the agent to 
achieve certain goals.

Any such policy network induces a probability distribution over the "path space"
of the world: starting from an initial state, the agent samples an action
according to the network and takes this action, and keeps repeating this until
the simulation is over. 

Suppose now we are given a policy network $W$, with the induced path space
distribution $P$. We may run many many parallel simulations by letting the
agent behave according to $W$, thereby drawing many samples from $P$, say
obtaining

$$\{p_1, p_2,\ldots, p_N\}$$

where $p_i\in\mathrm{supp}(P)$ is a particular "path" the agent took in 
simulation number $i$. Suppose now we can assign a quality score $q(p_i)$ to
each path denoting how well the agent did to achieve the particular goal we
want.

Now consider the "empirical distribution" on the path space obtained as follows:
Sample $i\in\{1,2,\ldots, N\}$ with probability proportional to $q(p_i)$ and
output $p_i$. Call this distribution over the paths $Q$. In words, $Q$ is a
reweigting (of an empirical sample) of $P$.

(One interesting thing is that while $P$ was Markovian with respect to world
state, $Q$ is not necessarily so.)

In a certain sense, this simulation we made revealed to us
$\mathbf{D}(Q\,\|\,P)$ bits of information about the shortcomings of our policy
network $N$, where $\mathbf{D}(\cdot\|\cdot)$ is the Kullback-Leibler
divergence.
For simplicity in notation, I will assume that in our world, all 
paths are the same length; though we can easily remove this assumption.
Writing

$$ \mathbf{D}(Q\,\|\,P) =
    \sum_{i=1}^{|p_0|} \mathbf{D}(Q_i\,|\,Q_{< i}\,\|\,P_i|P_{< i}),$$

this experiment gives us $N\times |p_0|$ data point with which we can improve
$W$ with SGD.

Doing this simulation and then SGD training and simulation with improved network
and so on alternately gives us a powerful way to discover extremely competitive
strategies.

### Why an XLA client in Rust
In a certain sense in RL we develop a better understanding about the world
by making many simulations with a more competent agent gradually. This results
in a self-reinforcing cycle, which leads to the name RL.
Making many
parallel simulations quickly involves two things

1. Feeding the world state to the network $W$ and getting back a distribution
on actions

2. Being able to run a time step of the "world" and our agent in the simulation.

It is precisely second item where Python makes things very slow, difficult, or
impossible in some cases.

This repo is my attempt at writing an XLA client in Rust so that one can 
perform 2. efficiently while still being able to do 1. quickly by offloading the
network inference to powerful accelerators such as Google Cloud GPUs or TPUs.

**Note:** DeepMind's `open_spiel` is a RL platform, and thus needs to solve the
same fast inference problem I outlined. They solve this by bringing up a full
TensorFlow session and running the XLA compilation through the `@tf.function`
mechanism. For the purposes of this project, that route is not ideal as 
TensorFlow session is a ginormous dependency that would require a build
cluster; further `@tf.function` interface currently does not seem to expose all
the compilation options XLA provides.
